# Neural Language Model â€” Assignment 2


## Overview
Word-level LSTM language model implemented from scratch using PyTorch. This repository contains code, notebooks, model checkpoints (linked separately via Google Drive), plots, and a short report demonstrating underfitting, overfitting, and best-fit experiments.


## Repository Structure


## ğŸš€ Overview  
This project implements a **Neural Language Model from scratch** using **PyTorch**.  
A **word-level LSTM** is trained on the provided dataset to predict the next word in a sequence.

This assignment demonstrates:

- âœ”ï¸ Underfitting  
- âœ”ï¸ Overfitting  
- âœ”ï¸ Best-fit model  
- âœ”ï¸ Training & validation loss graphs  
- âœ”ï¸ Perplexity evaluation  
- âœ”ï¸ Text generation samples  

---

## ğŸ“„ Dataset  
- The dataset used is:  
  **Pride and Prejudice â€” Jane Austen**  
- Tokenization: **Whitespace tokenization** 25000 
- Preprocessing:  
  - Lowercasing  
  - Removing newline characters  
  - Removing extra spaces  

---
## ğŸ§  Model Architecture (LSTM)

```
Embedding Layer (word-level)
â†’ 2-layer LSTM
â†’ Fully Connected Layer (vocab projection)
â†’ Softmax (through CrossEntropyLoss)
```

Hyperparameters:  
```
Embedding Size: 128
Hidden Size: 256
Sequence Length: 50
Batch Size: 64
Optimizer: Adam
Loss: CrossEntropyLoss
Gradient Clipping: 1.0
Early Stopping: patience = 3
```

---

## ğŸ§ª Experiments

### âœ”ï¸ **1. Underfitting Model**
- embed_dim = 128  
- hidden_dim = 64  
- num_layers = 1  
- epochs = 3  

### âœ”ï¸ **2. Overfitting Model**  
- embed_dim = 256  
- hidden_dim = 512  
- num_layers = 3  
- tiny dataset slice (3000 tokens)

### âœ”ï¸ **3. Best-Fit Model**  
- embed_dim = 128  
- hidden_dim = 256  
- num_layers = 2  
- full dataset

---

## ğŸ“Š Training vs Validation Loss Plots  
All plots are saved in Google Drive:

ğŸ‘‰ **Drive Folder:**  
`https://drive.google.com/drive/folders/PUT_YOUR_FOLDER_LINK_HERE`

Files included:  
- `underfit_loss.png`  
- `overfit_loss.png`  
- `bestfit_loss.png`  
- `combined_loss_plot.png`  


 ---

## ğŸ“‰ Final Metrics

| Model     | Val Loss | Perplexity |
|-----------|----------|------------|
| Underfit  | 6.9171   | 1009.38    |
| Overfit   | 9.0280   | 8333.23    |
| Best-Fit  | 7.3997   | 1635.63    |

---

## âœï¸ Sample Text Generation

**Prompt:** `Elizabeth`  
**Output (Best Model):**  
```
elizabeth had been a very time which he is not for a few of the whole man as her sister was not at the whole in the letter was
```

---

## â–¶ï¸ How to Run (Google Colab)

```
from google.colab import drive
drive.mount('/content/drive')

# Open notebook.ipynb and run all cells
```

OR run script:

```
python train_language_model.py --data_path <path> --out_dir ./out
```

---

## ğŸ“¦ Requirements

```
torch
numpy
pandas
matplotlib
```

---

## ğŸ“ Report  
The full detailed report is available in **report.md** inside this repository.

---

## ğŸ“¬ Contact  
venkata siva ch  
Email: venkatasivach15@gmail.com

---

# â­ Notes  
- Model checkpoints (.pth) are stored in Google Drive due to size limits.  


